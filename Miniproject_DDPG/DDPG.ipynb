{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a heuristic policy\n",
    "class HeuristicPendulumAgent():\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.agent_name='HeuristicPendulumAgent'\n",
    "\n",
    "    def get_agent_name(self):\n",
    "        return self.agent_name\n",
    "\n",
    "    def compute_action(self, state,fix_torque=0.1):\n",
    "         # When the pendulum is in the lower half of the domain (x<0)\n",
    "         if state[0]<0:\n",
    "             # applies a fixed torque in the same direction as the pendulum’s angular velocity\n",
    "             return fix_torque*np.sign(state[2])\n",
    "         # When the pendulum is in the higher half of the domain (x>0)\n",
    "         else:\n",
    "             # applies a fixed torque in the  opposite direction as the pendulum’s angular velocity\n",
    "             return -1*fix_torque*np.sign(state[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoState(env,agent,observation,total_reward_per_episode,agent_name='RandomAgent',fix_torque=0.1):\n",
    "    if agent_name=='HeuristicPendulumAgent':\n",
    "        action = agent.compute_action(observation,fix_torque)\n",
    "    else:\n",
    "        action = agent.compute_action(observation)\n",
    "    # print('action:',action)\n",
    "    normalized_env = NormalizedEnv(env)\n",
    "    normalized_action = normalized_env.action(action)\n",
    "    # print('normalized_action:',normalized_action)\n",
    "    observation, reward, terminated, truncated, info = env.step(normalized_action)\n",
    "    # print('observation:',observation)\n",
    "    # print('reward:',reward)\n",
    "    # print('terminated:',terminated)\n",
    "    total_reward_per_episode += reward\n",
    "    # print('total_reward:',total_reward)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    return observation, info, total_reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoEpisode(env,agent,agent_name='RandomAgent',fix_torque=0.1,num_state=200) :\n",
    "    observation, info = env.reset()\n",
    "    total_reward_per_episode=0\n",
    "    for _ in range(num_state):\n",
    "        observation, info, total_reward_per_episode=DoState(env,agent,observation,total_reward_per_episode,agent_name,fix_torque)\n",
    "        # print(\"total_reward_per_episode:\",total_reward_per_episode)\n",
    "    return total_reward_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "num_episode=10\n",
    "# Part 3: Report the average cumulative reward obtained by the heuristic policy\n",
    "# draw plot to compare the average cumulative reward obtained by the heuristic policy \n",
    "# with the reward of the random agent\n",
    "list_average_reward_HeuristicAgent=[]\n",
    "total_episodes_reward_HeuristicAgent = 0\n",
    "for episode in range(num_episode):\n",
    "    agent = HeuristicPendulumAgent(env)\n",
    "    total_reward_per_episode=DoEpisode(env, agent,agent.get_agent_name())\n",
    "    total_episodes_reward_HeuristicAgent += total_reward_per_episode\n",
    "    print('total_reward:', total_episodes_reward_HeuristicAgent)\n",
    "    average_reward = total_episodes_reward_HeuristicAgent / (episode + 1)\n",
    "    print('average_reward:', average_reward)\n",
    "    list_average_reward_HeuristicAgent.append(average_reward)\n",
    "\n",
    "list_average_reward_RandomAgent=[]\n",
    "total_episodes_reward_RandomAgent = 0\n",
    "for episode in range(num_episode):\n",
    "    agent = RandomAgent(env)\n",
    "    total_reward_per_episode=DoEpisode(env, agent)\n",
    "    total_episodes_reward_RandomAgent += total_reward_per_episode\n",
    "    print('total_reward:', total_episodes_reward_RandomAgent)\n",
    "    average_reward = total_episodes_reward_RandomAgent / (episode + 1)\n",
    "    print('average_reward:', average_reward)\n",
    "    list_average_reward_RandomAgent.append(average_reward)\n",
    "\n",
    "list_number_episodes = np.linspace(1,10,num=10)\n",
    "\n",
    "plt.plot(list_number_episodes,list_average_reward_HeuristicAgent,label= 'Average cumulative reward obtained by the heuristic policy')\n",
    "plt.plot(list_number_episodes,list_average_reward_RandomAgent,label= 'Average cumulative reward obtained by the random agent')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Average cumulative reward')\n",
    "plt.legend()\n",
    "plt.title('Compare the average cumulative reward obtained by the heuristic policy with the one obtained by the random agent')\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Report the average cumulative reward obtained by the heuristic policy\n",
    "# draw plot to show the impact of different amplitude of the fixed torque have on the reward\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "num_episode=10\n",
    "list_last_average_reward=[]\n",
    "list_fix_torque=np.linspace(0.1, 2.0, num=20)\n",
    "print(list_fix_torque)\n",
    "for fix_torque in list_fix_torque:\n",
    "    total_episodes_reward_HeuristicAgent=0\n",
    "    for episode in range(num_episode):\n",
    "        agent = HeuristicPendulumAgent(env)\n",
    "        total_reward_per_episode=DoEpisode(env,agent,agent.get_agent_name(),fix_torque)\n",
    "        total_episodes_reward_HeuristicAgent += total_reward_per_episode\n",
    "        print('total_reward:', total_episodes_reward_HeuristicAgent)\n",
    "        average_reward = total_episodes_reward_HeuristicAgent / (episode + 1)\n",
    "        print('average_reward:', average_reward)\n",
    "    list_last_average_reward.append(average_reward)\n",
    "plt.plot(list_fix_torque,list_last_average_reward,'-o')\n",
    "plt.xlabel(\"Fix Torque\")\n",
    "plt.ylabel(\"Average Reward (10 episodes)\")\n",
    "plt.title('Average Reward with differnt Fixed Torque (HeuristicPendulumAgent)')\n",
    "plt.xticks(list_fix_torque)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
