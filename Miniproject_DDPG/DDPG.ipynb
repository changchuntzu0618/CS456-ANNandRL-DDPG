{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from helpers import NormalizedEnv, RandomAgent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Heuristic Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a heuristic policy\n",
    "class HeuristicPendulumAgent():\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.agent_name='HeuristicPendulumAgent'\n",
    "\n",
    "    def get_agent_name(self):\n",
    "        return self.agent_name\n",
    "\n",
    "    def compute_action(self, state,fix_torque=0.1):\n",
    "         # When the pendulum is in the lower half of the domain (x<0)\n",
    "         if state[0]<0:\n",
    "             # applies a fixed torque in the same direction as the pendulum’s angular velocity\n",
    "             return fix_torque*np.sign(state[2])\n",
    "         # When the pendulum is in the higher half of the domain (x>0)\n",
    "         else:\n",
    "             # applies a fixed torque in the  opposite direction as the pendulum’s angular velocity\n",
    "             return -1*fix_torque*np.sign(state[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoState(env,agent,observation,total_reward_per_episode,agent_name='RandomAgent',fix_torque=0.1):\n",
    "    if agent_name=='HeuristicPendulumAgent':\n",
    "        action = agent.compute_action(observation,fix_torque)\n",
    "    else:\n",
    "        action = agent.compute_action(observation)\n",
    "    # print('action:',action)\n",
    "    normalized_env = NormalizedEnv(env)\n",
    "    normalized_action = normalized_env.action(action)\n",
    "    # print('normalized_action:',normalized_action)\n",
    "    observation, reward, terminated, truncated, info = env.step(normalized_action)\n",
    "    # print('observation:',observation)\n",
    "    # print('reward:',reward)\n",
    "    # print('terminated:',terminated)\n",
    "    total_reward_per_episode += reward\n",
    "    # print('total_reward:',total_reward)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    return observation, info, total_reward_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoEpisode(env,agent,agent_name='RandomAgent',fix_torque=0.1,num_state=200) :\n",
    "    observation, info = env.reset()\n",
    "    total_reward_per_episode=0\n",
    "    for _ in range(num_state):\n",
    "        observation, info, total_reward_per_episode=DoState(env,agent,observation,total_reward_per_episode,agent_name,fix_torque)\n",
    "        # print(\"total_reward_per_episode:\",total_reward_per_episode)\n",
    "    return total_reward_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "num_episode=10\n",
    "# Part 3: Report the average cumulative reward obtained by the heuristic policy\n",
    "# draw plot to compare the average cumulative reward obtained by the heuristic policy \n",
    "# with the reward of the random agent\n",
    "list_average_reward_HeuristicAgent=[]\n",
    "total_episodes_reward_HeuristicAgent = 0\n",
    "for episode in range(num_episode):\n",
    "    agent = HeuristicPendulumAgent(env)\n",
    "    total_reward_per_episode=DoEpisode(env, agent,agent.get_agent_name())\n",
    "    total_episodes_reward_HeuristicAgent += total_reward_per_episode\n",
    "    print('total_reward:', total_episodes_reward_HeuristicAgent)\n",
    "    average_reward = total_episodes_reward_HeuristicAgent / (episode + 1)\n",
    "    print('average_reward:', average_reward)\n",
    "    list_average_reward_HeuristicAgent.append(average_reward)\n",
    "\n",
    "list_average_reward_RandomAgent=[]\n",
    "total_episodes_reward_RandomAgent = 0\n",
    "for episode in range(num_episode):\n",
    "    agent = RandomAgent(env)\n",
    "    total_reward_per_episode=DoEpisode(env, agent)\n",
    "    total_episodes_reward_RandomAgent += total_reward_per_episode\n",
    "    print('total_reward:', total_episodes_reward_RandomAgent)\n",
    "    average_reward = total_episodes_reward_RandomAgent / (episode + 1)\n",
    "    print('average_reward:', average_reward)\n",
    "    list_average_reward_RandomAgent.append(average_reward)\n",
    "\n",
    "list_number_episodes = np.linspace(1,10,num=10)\n",
    "\n",
    "plt.plot(list_number_episodes,list_average_reward_HeuristicAgent,label= 'Average cumulative reward obtained by the heuristic policy')\n",
    "plt.plot(list_number_episodes,list_average_reward_RandomAgent,label= 'Average cumulative reward obtained by the random agent')\n",
    "plt.xlabel('Number of episodes')\n",
    "plt.ylabel('Average cumulative reward')\n",
    "plt.legend()\n",
    "plt.title('Compare the average cumulative reward obtained by the heuristic policy with the one obtained by the random agent')\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Report the average cumulative reward obtained by the heuristic policy\n",
    "# draw plot to show the impact of different amplitude of the fixed torque have on the reward\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "num_episode=10\n",
    "list_last_average_reward=[]\n",
    "list_fix_torque=np.linspace(0.1, 2.0, num=20)\n",
    "print(list_fix_torque)\n",
    "for fix_torque in list_fix_torque:\n",
    "    total_episodes_reward_HeuristicAgent=0\n",
    "    for episode in range(num_episode):\n",
    "        agent = HeuristicPendulumAgent(env)\n",
    "        total_reward_per_episode=DoEpisode(env,agent,agent.get_agent_name(),fix_torque)\n",
    "        total_episodes_reward_HeuristicAgent += total_reward_per_episode\n",
    "        print('total_reward:', total_episodes_reward_HeuristicAgent)\n",
    "        average_reward = total_episodes_reward_HeuristicAgent / (episode + 1)\n",
    "        print('average_reward:', average_reward)\n",
    "    list_last_average_reward.append(average_reward)\n",
    "plt.plot(list_fix_torque,list_last_average_reward,'-o')\n",
    "plt.xlabel(\"Fix Torque\")\n",
    "plt.ylabel(\"Average Reward (10 episodes)\")\n",
    "plt.title('Average Reward with differnt Fixed Torque (HeuristicPendulumAgent)')\n",
    "plt.xticks(list_fix_torque)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q function of the heuristic policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Replay Buffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        # max_size: how many transitions replay buffer can store at most\n",
    "        self.max_size = max_size\n",
    "        self.total_trainsition = []\n",
    "\n",
    "    def add(self, transition):\n",
    "        # transition: a tuple of (state, action, reward, next_state, done)\n",
    "\n",
    "        # check if the replay buffer is full\n",
    "        if len(self.total_trainsition) >= self.max_size:\n",
    "            print(\"The replay buffer is full.-> remove the oldest transition\")\n",
    "            # remove the oldest transition\n",
    "            self.total_trainsition.pop(0)\n",
    "        # add transition to replay buffer\n",
    "        self.total_trainsition.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # batch_size: how many transitions will be sampled\n",
    "        # return a batch of transitions\n",
    "        \n",
    "        # check if the replay buffer is empty\n",
    "        if len(self.total_trainsition) == 0:\n",
    "            print(\"The replay buffer is empty.\")\n",
    "            return None\n",
    "        \n",
    "        # check if the replay buffer has enough transitions\n",
    "        if len(self.total_trainsition) < batch_size:\n",
    "            print(\"The replay buffer does not have enough transitions.\")\n",
    "            return None\n",
    "        \n",
    "        # sample batch_size transitions from the replay buffer randomly\n",
    "        return random.sample(self.total_trainsition, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the class QNetwork\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim=4, output_dim=1, hidden_dim=32):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # state_dim: dimension of state -> 3 elements of the state and 1 of the action\n",
    "        # hidden_dim: dimension of hidden layer -> 32 nodes\n",
    "        # output_dim: dimension of action -> a scalar value (the expected cumulative reward)\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # state: state -> 3 elements of the state and 1 of the action\n",
    "        # return expected cumulative reward \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
